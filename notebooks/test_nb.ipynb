{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jluesch/Documents/GitHub/plankton-dinov2/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/home/jluesch/Documents/GitHub/plankton-dinov2/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/home/jluesch/Documents/GitHub/plankton-dinov2/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from dinov2.fsdp import FSDPCheckpointer\n",
    "from dinov2.models.vision_transformer import DinoVisionTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_0 = '/home/jluesch/Documents/output_dir/run_05012024_141438/model_final.rank_0.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "50949\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import socket\n",
    "\n",
    "os.environ['RANK']='0'\n",
    "os.environ['WORLD_SIZE']='2'\n",
    "os.environ['MASTER_ADDR']='127.0.0.1'\n",
    "os.environ['MASTER_PORT']='0'\n",
    "\n",
    "with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "    # A \"\" host address means INADDR_ANY i.e. binding to all interfaces.\n",
    "    # Note this is not compatible with IPv6.\n",
    "    s.bind((\"\", 0))\n",
    "    port = s.getsockname()[1]\n",
    "\n",
    "print(111)\n",
    "print(port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['MASTER_PORT'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtdist\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_process_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnccl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/dinov2/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:920\u001b[0m, in \u001b[0;36minit_process_group\u001b[0;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)\u001b[0m\n\u001b[1;32m    916\u001b[0m     barrier()\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;66;03m# Use store based barrier here since barrier() used a bunch of\u001b[39;00m\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;66;03m# default devices and messes up NCCL internal state.\u001b[39;00m\n\u001b[0;32m--> 920\u001b[0m     \u001b[43m_store_based_barrier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/dinov2/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:459\u001b[0m, in \u001b[0;36m_store_based_barrier\u001b[0;34m(rank, store, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m         log_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timedelta(seconds\u001b[38;5;241m=\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start)) \u001b[38;5;241m>\u001b[39m timeout:\n\u001b[0;32m--> 459\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    460\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimed out initializing process group in store based barrier on \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    461\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, for key: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m (world_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, worker_count=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, timeout=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    462\u001b[0m                 rank, store_key, world_size, worker_count, timeout\n\u001b[1;32m    463\u001b[0m             )\n\u001b[1;32m    464\u001b[0m         )\n\u001b[1;32m    466\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRank \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Completed store-based barrier for key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstore_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworld_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m nodes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)"
     ]
    }
   ],
   "source": [
    "import torch.distributed as tdist\n",
    "\n",
    "tdist.init_process_group(backend='nccl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(ckpt_0, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_t =[\"Amphidinium_sp\",\"Asterionellopsis\",\"Bacillaria\",\"Bidulphia\",\"Cerataulina\",\"Cerataulina_flagellate\",\"Ceratium\",\"Chaetoceros\",\"Chaetoceros_didymus\",\"Chaetoceros_didymus_flagellate\",\"Chaetoceros_flagellate\",\"Chaetoceros_other\",\"Chaetoceros_pennate\",\"Chrysochromulina\",\"Ciliate_mix\",\"Cochlodinium\",\"Corethron\",\"Coscinodiscus\",\"Cylindrotheca\",\"DactFragCerataul\",\"Dactyliosolen\",\"Delphineis\",\"Dictyocha\",\"Didinium_sp\",\"Dinobryon\",\"Dinophysis\",\"Ditylum\",\"Ditylum_parasite\",\"Emiliania_huxleyi\",\"Ephemera\",\"Eucampia\",\"Euglena\",\"Euplotes_sp\",\"G_delicatula_detritus\",\"G_delicatula_external_parasite\",\"G_delicatula_parasite\",\"Gonyaulax\",\"Guinardia_delicatula\",\"Guinardia_flaccida\",\"Guinardia_striata\",\"Gyrodinium\",\"Hemiaulus\",\"Heterocapsa_triquetra\",\"Katodinium_or_Torodinium\",\"Laboea_strobila\",\"Lauderia\",\"Leegaardiella_ovalis\",\"Leptocylindrus\",\"Leptocylindrus_mediterraneus\",\"Licmophora\",\"Mesodinium_sp\",\"Odontella\",\"Paralia\",\"Parvicorbicula_socialis\",\"Phaeocystis\",\"Pleuronema_sp\",\"Pleurosigma\",\"Prorocentrum\",\"Proterythropsis_sp\",\"Protoperidinium\",\"Pseudochattonella_farcimen\",\"Pseudonitzschia\",\"Pyramimonas_longicauda\",\"Rhizosolenia\",\"Skeletonema\",\"Stephanopyxis\",\"Strobilidium_morphotype1\",\"Strombidium_capitatum\",\"Strombidium_conicum\",\"Strombidium_inclinatum\",\"Strombidium_morphotype1\",\"Strombidium_morphotype2\",\"Strombidium_oculatum\",\"Strombidium_wulffi\",\"Thalassionema\",\"Thalassiosira\",\"Thalassiosira_dirty\",\"Tiarina_fusus\",\"Tintinnid\",\"Tontonia_appendiculariformis\",\"Tontonia_gracillima\",\"amoeba\",\"bad\",\"bead\",\"bubble\",\"clusterflagellate\",\"detritus\",\"diatom_flagellate\",\"dino30\",\"dino_large1\",\"flagellate_sp3\",\"kiteflagellates\",\"mix\",\"mix_elongated\",\"other_interaction\",\"pennate\",\"pennate_morphotype1\",\"pennates_on_diatoms\",\"pollen\",\"spore\",\"zooplankton\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_v = [\"Akashiwo\",\"Amphidinium_sp\",\"Asterionellopsis\",\"Bacillaria\",\"Bidulphia\",\"Cerataulina\",\"Cerataulina_flagellate\",\"Ceratium\",\"Chaetoceros\",\"Chaetoceros_didymus\",\"Chaetoceros_didymus_flagellate\",\"Chaetoceros_flagellate\",\"Chaetoceros_other\",\"Chaetoceros_pennate\",\"Chrysochromulina\",\"Ciliate_mix\",\"Cochlodinium\",\"Corethron\",\"Coscinodiscus\",\"Cylindrotheca\",\"DactFragCerataul\",\"Dactyliosolen\",\"Delphineis\",\"Dictyocha\",\"Didinium_sp\",\"Dinobryon\",\"Dinophysis\",\"Ditylum\",\"Ditylum_parasite\",\"Emiliania_huxleyi\",\"Ephemera\",\"Eucampia\",\"Euglena\",\"Euplotes_sp\",\"G_delicatula_detritus\",\"G_delicatula_external_parasite\",\"G_delicatula_parasite\",\"Guinardia_delicatula\",\"Guinardia_flaccida\",\"Guinardia_striata\",\"Gyrodinium\",\"Hemiaulus\",\"Heterocapsa_triquetra\",\"Katodinium_or_Torodinium\",\"Laboea_strobila\",\"Lauderia\",\"Leegaardiella_ovalis\",\"Leptocylindrus\",\"Leptocylindrus_mediterraneus\",\"Licmophora\",\"Mesodinium_sp\",\"Odontella\",\"Paralia\",\"Parvicorbicula_socialis\",\"Phaeocystis\",\"Pleuronema_sp\",\"Pleurosigma\",\"Prorocentrum\",\"Proterythropsis_sp\",\"Protoperidinium\",\"Pseudochattonella_farcimen\",\"Pseudonitzschia\",\"Pyramimonas_longicauda\",\"Rhizosolenia\",\"Skeletonema\",\"Stephanopyxis\",\"Strobilidium_morphotype1\",\"Strombidium_capitatum\",\"Strombidium_conicum\",\"Strombidium_inclinatum\",\"Strombidium_morphotype1\",\"Strombidium_morphotype2\",\"Strombidium_oculatum\",\"Strombidium_wulffi\",\"Thalassionema\",\"Thalassiosira\",\"Thalassiosira_dirty\",\"Tiarina_fusus\",\"Tintinnid\",\"Tontonia_appendiculariformis\",\"Tontonia_gracillima\",\"amoeba\",\"bad\",\"bead\",\"clusterflagellate\",\"detritus\",\"diatom_flagellate\",\"dino30\",\"dino_large1\",\"flagellate_sp3\",\"kiteflagellates\",\"mix\",\"mix_elongated\",\"other_interaction\",\"pennate\",\"pennate_morphotype1\",\"pennates_on_diatoms\",\"pollen\",\"spore\",\"zooplankton\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Gonyaulax', 'bubble'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(list_t).difference(set(list_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/jluesch/Documents/data/plankton'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011-VAL.hdf5 94\n",
      "2007-TRAIN.hdf5 88\n",
      "2013-VAL.hdf5 93\n",
      "2008-TRAIN.hdf5 92\n",
      "2009-TRAIN.hdf5 94\n",
      "2010-VAL.hdf5 97\n",
      "2012-VAL.hdf5 95\n",
      "2014-VAL.hdf5 94\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc_ids = []\n",
    "acc_names = []\n",
    "\n",
    "all_files_entries = dict()\n",
    "class_names_dict = dict()\n",
    "for hdf5_file in files:\n",
    "    print(hdf5_file, end=\" \")\n",
    "    all_files_entries[hdf5_file] = []\n",
    "    class_ids = []\n",
    "    class_names = []\n",
    "    hdf5_file_path = os.path.join(root, hdf5_file)\n",
    "    file = h5py.File(hdf5_file_path, 'r')\n",
    "\n",
    "    # Read the JSON string from the 'file_index' dataset\n",
    "    file_index_json = file['file_index'][()]\n",
    "    file_index = json.loads(file_index_json)\n",
    "\n",
    "    # Add the HDF5 file name to each entry and accumulate the file entries\n",
    "    for entry in file_index['files']:\n",
    "        entry['hdf5_file'] = hdf5_file_path  # Add the HDF5 file name to the entry\n",
    "        all_files_entries[hdf5_file].append(entry)\n",
    "        class_id = entry['class_id']\n",
    "        class_str = entry['class_str']\n",
    "        if class_id not in class_ids:\n",
    "            class_ids.append(class_id)\n",
    "            class_names.append(class_str)\n",
    "\n",
    "        if class_id not in acc_ids:\n",
    "            acc_ids.append(class_id)\n",
    "            acc_names.append(class_str)\n",
    "    class_names_dict[hdf5_file] = class_names\n",
    "\n",
    "    print(len(class_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 0, 'path': '2014_Akashiwo_IFCB1_2014_277_165430_04321.png', 'class_str': 'Akashiwo', 'class_id': 101, 'hdf5_file': '/home/jluesch/Documents/data/plankton/2014-VAL.hdf5'}\n",
      "{'index': 1, 'path': '2014_amoeba_IFCB1_2014_188_215702_06173.png', 'class_str': 'amoeba', 'class_id': 0, 'hdf5_file': '/home/jluesch/Documents/data/plankton/2014-VAL.hdf5'}\n",
      "{'index': 2, 'path': '2014_amoeba_IFCB1_2014_188_222013_06882.png', 'class_str': 'amoeba', 'class_id': 0, 'hdf5_file': '/home/jluesch/Documents/data/plankton/2014-VAL.hdf5'}\n",
      "{'index': 3, 'path': '2014_amoeba_IFCB1_2014_188_224330_01921.png', 'class_str': 'amoeba', 'class_id': 0, 'hdf5_file': '/home/jluesch/Documents/data/plankton/2014-VAL.hdf5'}\n",
      "{'index': 4, 'path': '2014_amoeba_IFCB1_2014_270_212839_05223.png', 'class_str': 'amoeba', 'class_id': 0, 'hdf5_file': '/home/jluesch/Documents/data/plankton/2014-VAL.hdf5'}\n",
      "{'index': 5, 'path': '2014_amoeba_IFCB1_2014_270_212839_05436.png', 'class_str': 'amoeba', 'class_id': 0, 'hdf5_file': '/home/jluesch/Documents/data/plankton/2014-VAL.hdf5'}\n",
      "{'index': 6, 'path': '2014_amoeba_IFCB1_2014_270_212839_06793.png', 'class_str': 'amoeba', 'class_id': 0, 'hdf5_file': '/home/jluesch/Documents/data/plankton/2014-VAL.hdf5'}\n",
      "{'index': 7, 'path': '2014_amoeba_IFCB1_2014_277_165430_02754.png', 'class_str': 'amoeba', 'class_id': 0, 'hdf5_file': '/home/jluesch/Documents/data/plankton/2014-VAL.hdf5'}\n",
      "{'index': 8, 'path': '2014_amoeba_IFCB1_2014_277_165430_06762.png', 'class_str': 'amoeba', 'class_id': 0, 'hdf5_file': '/home/jluesch/Documents/data/plankton/2014-VAL.hdf5'}\n",
      "{'index': 9, 'path': '2014_amoeba_IFCB1_2014_277_171744_01093.png', 'class_str': 'amoeba', 'class_id': 0, 'hdf5_file': '/home/jluesch/Documents/data/plankton/2014-VAL.hdf5'}\n"
     ]
    }
   ],
   "source": [
    "class_count = dict()\n",
    "for entry in entries[:10]:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['2011-VAL.hdf5', '2007-TRAIN.hdf5', '2013-VAL.hdf5', '2008-TRAIN.hdf5', '2009-TRAIN.hdf5', '2010-VAL.hdf5', '2012-VAL.hdf5', '2014-VAL.hdf5'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = set()\n",
    "for v in class_names_dict.values():\n",
    "    all_names = all_names.union(set(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    }
   ],
   "source": [
    "print(len(all_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011-VAL.hdf5  missing : {'Gonyaulax', 'Tiarina_fusus', 'Chaetoceros_didymus_flagellate', 'bubble', 'pollen', 'Akashiwo', 'Tontonia_appendiculariformis', 'bead'}\n",
      "2007-TRAIN.hdf5  missing : {'Gonyaulax', 'Cochlodinium', 'dino_large1', 'Chaetoceros_didymus_flagellate', 'Euplotes_sp', 'bubble', 'pennate_morphotype1', 'Akashiwo', 'Tontonia_appendiculariformis', 'Hemiaulus', 'bead', 'Emiliania_huxleyi', 'Cerataulina_flagellate', 'Bacillaria'}\n",
      "2013-VAL.hdf5  missing : {'Gonyaulax', 'Tiarina_fusus', 'Didinium_sp', 'bubble', 'Akashiwo', 'Hemiaulus', 'kiteflagellates', 'Bidulphia', 'Leptocylindrus_mediterraneus'}\n",
      "2008-TRAIN.hdf5  missing : {'Cochlodinium', 'dino_large1', 'Didinium_sp', 'Akashiwo', 'Pseudochattonella_farcimen', 'kiteflagellates', 'bead', 'Emiliania_huxleyi', 'Leptocylindrus_mediterraneus', 'Bacillaria'}\n",
      "2009-TRAIN.hdf5  missing : {'Chaetoceros_didymus_flagellate', 'bubble', 'Akashiwo', 'Hemiaulus', 'kiteflagellates', 'Emiliania_huxleyi', 'Leptocylindrus_mediterraneus', 'Bacillaria'}\n",
      "2010-VAL.hdf5  missing : {'Tiarina_fusus', 'Akashiwo', 'Hemiaulus', 'Bidulphia', 'bead'}\n",
      "2012-VAL.hdf5  missing : {'Gonyaulax', 'Didinium_sp', 'bubble', 'Odontella', 'Stephanopyxis', 'Bacillaria', 'Lauderia'}\n",
      "2014-VAL.hdf5  missing : {'Gonyaulax', 'Tiarina_fusus', 'bubble', 'Tontonia_appendiculariformis', 'Hemiaulus', 'Bidulphia', 'Strombidium_wulffi', 'Bacillaria'}\n"
     ]
    }
   ],
   "source": [
    "for k, v in class_names_dict.items():\n",
    "    print(k, ' missing :', all_names.difference(set(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "from dinov2.data import SamplerType, make_data_loader, make_dataset\n",
    "from dinov2.data import collate_data_and_cast, DataAugmentationDINO, MaskingGenerator\n",
    "import dinov2.distributed as distributed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.is_initialized())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#torch.cuda.set_device(0)\n",
    "print(torch.cuda.is_initialized())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "Dataset kwargs {'split': <_SplitHDF5Dataset.TRAIN: 'train'>, 'root': '/home/jluesch/Documents/data/plankton/2007-TRAIN.hdf5', 'extra': '*', 'do_short_run': False}\n",
      "root: /home/jluesch/Documents/data/plankton/2007-TRAIN.hdf5, extra_root: *, extra_path: /home/jluesch/Documents/data/plankton/2007-TRAIN.hdf5\n",
      "Datasets file list:  ['/home/jluesch/Documents/data/plankton/2007-TRAIN.hdf5']\n",
      "#unique_class_ids: _SplitHDF5Dataset.TRAIN, 88\n",
      "#unique_class_names: ['Amphidinium_sp' 'Asterionellopsis' 'Bidulphia' 'Cerataulina' 'Ceratium'\n",
      " 'Chaetoceros' 'Chaetoceros_didymus' 'Chaetoceros_flagellate'], 88\n",
      "#samples: 289020\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "train_ds_path = 'HDF5Dataset:split=TRAIN:root=/home/jluesch/Documents/data/plankton/2007-TRAIN.hdf5:extra=*'\n",
    "img_size = 224\n",
    "patch_size = 14\n",
    "n_tokens = (img_size // patch_size) ** 2\n",
    "inputs_dtype = torch.half\n",
    "\n",
    "print(torch.cuda.is_initialized())\n",
    "\n",
    "mask_generator = MaskingGenerator(\n",
    "    input_size=(img_size // patch_size, img_size // patch_size),\n",
    "    max_num_patches=0.5 * img_size // patch_size * img_size // patch_size,\n",
    ")\n",
    "\n",
    "print(torch.cuda.is_initialized())\n",
    "\n",
    "data_transform = DataAugmentationDINO(\n",
    "    (0.32, 1.0),\n",
    "    (0.05, 0.32),\n",
    "    8,\n",
    "    global_crops_size=224,\n",
    "    local_crops_size=96,\n",
    "    do_transform_on_gpu=True,\n",
    ")\n",
    "\n",
    "print(torch.cuda.is_initialized())\n",
    "\n",
    "collate_fn = partial(\n",
    "    collate_data_and_cast,\n",
    "    mask_ratio_tuple=(0.1, 0.5),\n",
    "    mask_probability=0.5,\n",
    "    n_tokens=n_tokens,\n",
    "    mask_generator=mask_generator,\n",
    "    dtype=inputs_dtype,\n",
    ")\n",
    "\n",
    "print(torch.cuda.is_initialized())\n",
    "\n",
    "# setup data loader\n",
    "dataset = make_dataset(\n",
    "dataset_str=train_ds_path,\n",
    "transform=data_transform,\n",
    "target_transform=lambda _: (),\n",
    ")\n",
    "\n",
    "print(f'#samples: {len(dataset)}')\n",
    "print(torch.cuda.is_initialized())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jluesch/Documents/GitHub/plankton-dinov2/notebooks/../dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/home/jluesch/Documents/GitHub/plankton-dinov2/notebooks/../dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/home/jluesch/Documents/GitHub/plankton-dinov2/notebooks/../dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jluesch/micromamba/envs/dinov2/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/jluesch/micromamba/envs/dinov2/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jluesch/micromamba/envs/dinov2/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jluesch/Documents/GitHub/plankton-dinov2/notebooks/../dinov2/data/datasets/extended.py\", line 38, in __getitem__\n    image, target = self.transforms(image, target)\n  File \"/home/jluesch/micromamba/envs/dinov2/lib/python3.9/site-packages/torchvision/datasets/vision.py\", line 94, in __call__\n    input = self.transform(input)\n  File \"/home/jluesch/Documents/GitHub/plankton-dinov2/notebooks/../dinov2/data/augmentations.py\", line 112, in __call__\n    image = self.to_cuda_tensor(image)\n  File \"/home/jluesch/Documents/GitHub/plankton-dinov2/notebooks/../dinov2/data/augmentations.py\", line 138, in to_cuda_tensor\n    return image.to(device='cuda')\n  File \"/home/jluesch/micromamba/envs/dinov2/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 235, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m batch_delta_ts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     33\u001b[0m batch_start_t \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mtimestamp()\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, el \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_initialized())\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m el\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/micromamba/envs/dinov2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/micromamba/envs/dinov2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/dinov2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/micromamba/envs/dinov2/lib/python3.9/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jluesch/micromamba/envs/dinov2/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/jluesch/micromamba/envs/dinov2/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jluesch/micromamba/envs/dinov2/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jluesch/Documents/GitHub/plankton-dinov2/notebooks/../dinov2/data/datasets/extended.py\", line 38, in __getitem__\n    image, target = self.transforms(image, target)\n  File \"/home/jluesch/micromamba/envs/dinov2/lib/python3.9/site-packages/torchvision/datasets/vision.py\", line 94, in __call__\n    input = self.transform(input)\n  File \"/home/jluesch/Documents/GitHub/plankton-dinov2/notebooks/../dinov2/data/augmentations.py\", line 112, in __call__\n    image = self.to_cuda_tensor(image)\n  File \"/home/jluesch/Documents/GitHub/plankton-dinov2/notebooks/../dinov2/data/augmentations.py\", line 138, in to_cuda_tensor\n    return image.to(device='cuda')\n  File \"/home/jluesch/micromamba/envs/dinov2/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 235, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    }
   ],
   "source": [
    "from dinov2.models.vision_transformer import vit_small\n",
    "\n",
    "\n",
    "batch_size_list = [64]\n",
    "num_workers_list = [1]\n",
    "MAX_NB_SAMPLE = 5\n",
    "verbose=False\n",
    "\n",
    "#model = vit_small(patch_size=16, num_register_tokens=0)\n",
    "for num_workers in num_workers_list:\n",
    "    for batch_size in batch_size_list:\n",
    "\n",
    "        # sampler_type = SamplerType.INFINITE\n",
    "        sampler_type = SamplerType.SHARDED_INFINITE\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "        print(torch.cuda.is_initialized())\n",
    "\n",
    "        # do cuda distributed init w single gpu\n",
    "        start_t = datetime.now().timestamp()\n",
    "\n",
    "        batch_delta_ts = []\n",
    "        batch_start_t = datetime.now().timestamp()\n",
    "        for i, el in enumerate(data_loader):\n",
    "            print(torch.cuda.is_initialized())\n",
    "\n",
    "            for v in el.values():\n",
    "                if torch.is_tensor(v):\n",
    "                    v=v.cuda()\n",
    "            print(el['collated_global_crops'].shape, el['collated_global_crops'].dtype)\n",
    "\n",
    "            batch_end_t = datetime.now().timestamp()\n",
    "\n",
    "            batch_delta_t = (batch_end_t-batch_start_t)\n",
    "            batch_delta_ts.append(batch_delta_t)\n",
    "\n",
    "            if verbose or i%100 == 0:\n",
    "                print(f'{i}, {i*batch_size}: {batch_delta_t:.5}')\n",
    "                for v in el.values():\n",
    "                    print(el['collated_masks'].cuda().device, end=\" \")\n",
    "                print()\n",
    "            if verbose:\n",
    "                print(el['collated_masks'].shape)\n",
    "                print(el['mask_indices_list'].shape)\n",
    "                print(el['masks_weight'].shape)\n",
    "                print(el['upperbound'], el['n_masked_patches'])\n",
    "\n",
    "            batch_start_t = datetime.now().timestamp()\n",
    "\n",
    "            if i * batch_size >= MAX_NB_SAMPLE:\n",
    "                end_t = datetime.now().timestamp()\n",
    "                break\n",
    "\n",
    "\n",
    "        total_t = end_t - start_t\n",
    "        mean_batch = np.mean(batch_delta_ts)\n",
    "        std_batch = np.std(batch_delta_ts)\n",
    "        max_batch = np.max(batch_delta_ts)\n",
    "        min_batch = np.min(batch_delta_ts)\n",
    "        print(f'BATCH SIZE {batch_size}, NUM WORKERS {num_workers}, TIME TOTAL: {total_t:.5}')\n",
    "        print(f'PER BATCH: {mean_batch:.5} ± {std_batch:.5}, max {max_batch:.5}, min {min_batch:.5}')\n",
    "        print(f'TIME PER SAMPLE: {mean_batch/batch_size:.5} ± {std_batch/batch_size:.5}, max {max_batch/batch_size:.5}, min {min_batch/batch_size:.5}')\n",
    "        print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from torchvision import transforms\n",
    "\n",
    "im = PIL.Image.new(mode=\"RGB\", size=(4, 4), color = (153, 153, 255))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[153 153 255]\n",
      "  [153 153 255]\n",
      "  [153 153 255]\n",
      "  [153 153 255]]\n",
      "\n",
      " [[153 153 255]\n",
      "  [153 153 255]\n",
      "  [153 153 255]\n",
      "  [153 153 255]]\n",
      "\n",
      " [[153 153 255]\n",
      "  [153 153 255]\n",
      "  [153 153 255]\n",
      "  [153 153 255]]\n",
      "\n",
      " [[153 153 255]\n",
      "  [153 153 255]\n",
      "  [153 153 255]\n",
      "  [153 153 255]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6000, 0.6000, 0.6000, 0.6000],\n",
       "         [0.6000, 0.6000, 0.6000, 0.6000],\n",
       "         [0.6000, 0.6000, 0.6000, 0.6000],\n",
       "         [0.6000, 0.6000, 0.6000, 0.6000]],\n",
       "\n",
       "        [[0.6000, 0.6000, 0.6000, 0.6000],\n",
       "         [0.6000, 0.6000, 0.6000, 0.6000],\n",
       "         [0.6000, 0.6000, 0.6000, 0.6000],\n",
       "         [0.6000, 0.6000, 0.6000, 0.6000]],\n",
       "\n",
       "        [[1.0000, 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000, 1.0000]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.array(im))\n",
    "tt = transforms.ToTensor()\n",
    "tt(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class AugmentationType(Enum):\n",
    "    KORNIA_GPU = \"kornia_gpu\"\n",
    "    TORCHV_CPU = \"torchvision_cpu\"\n",
    "    TORCHV_GPU = \"torchvision_gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'kornia_gpu' == AugmentationType.KORNIA_GPU.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.zeros((2,32,44,44)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32, 44, 44])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(a ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--f'], dest='f', nargs=None, const=None, default='aaa', type=None, choices=None, required=False, help='path to config file', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\"DINOv2 training\")\n",
    "parser.add_argument(\"--config\", default=\"aaa\", help=\"path to config file\")\n",
    "parser.add_argument(\"--f\", default=\"aaa\", help=\"path to config file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.config = 'ssss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "dinov2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
